{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fbbed8e",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Movie Reviews via TMDB API\n",
    "**Author:** Logan Ash  \n",
    "**Date:** 2025‑05‑07  \n",
    "\n",
    "## Introduction\n",
    "Using the free TMDB (The Movie Database) API, we collect at least 60 user reviews for *The Shawshank Redemption* (TMDB movie ID 278). We clean the text, run sentiment analysis with TextBlob’s default analyzer **and** the NaiveBayesAnalyzer, visualise the sentiment distribution with donut charts, remove stop‑words, create a WordCloud of the 20 most frequent words, and finish with insights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d21c44c",
   "metadata": {},
   "source": [
    "## 1  Install & prepare dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5c66c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tmdbv3api textblob wordcloud nltk matplotlib --quiet\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e12538",
   "metadata": {},
   "source": [
    "## 2  Imports & TMDB API setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a1e4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tmdbv3api import TMDb, Movie\n",
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "\n",
    "# --- TMDB credentials (provided by user) ---\n",
    "tmdb = TMDb()\n",
    "tmdb.api_key = '4253c5a7dc926b08aa0d781c0136be78'\n",
    "tmdb.access_token = 'eyJhbGciOiJIUzI1NiJ9.eyJhdWQiOiI0MjUzYzVhN2RjOTI2YjA4YWEwZDc4MWMwMTM2YmU3OCIsIm5iZiI6MTc0NjU5NDIxNS43MDUsInN1YiI6IjY4MWFlOWE3ZTlhYTk5ZmM5OTgwOTMxNyIsInNjb3BlcyI6WyJhcGlfcmVhZCJdLCJ2ZXJzaW9uIjoxfQ.X6--msBkZCKseRid1_YcKNgIQWGGHoNcexgsqlk0btk'\n",
    "\n",
    "movie = Movie()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1095bad5",
   "metadata": {},
   "source": [
    "## 3  Fetch at least 60 reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c3aca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tmdbv3api.tmdb import TMDbException\n",
    "\n",
    "movie_id = 278  # The Shawshank Redemption\n",
    "reviews = []\n",
    "page = 1\n",
    "\n",
    "while len(reviews) < 60 and page <= 500:\n",
    "    try:\n",
    "        resp = movie.reviews(movie_id, page=page) if page > 1 else movie.reviews(movie_id)\n",
    "    except TMDbException as e:\n",
    "        print(f'Stopping fetch due to TMDbException: {e}')\n",
    "        break\n",
    "\n",
    "    if not resp:\n",
    "        break\n",
    "\n",
    "    for r in resp:\n",
    "        if hasattr(r, 'content') and r.content:\n",
    "            reviews.append(r.content)\n",
    "            if len(reviews) >= 60:\n",
    "                break\n",
    "\n",
    "    page += 1\n",
    "\n",
    "print(f'Collected {len(reviews)} reviews')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41416e6a",
   "metadata": {},
   "source": [
    "## 4  Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a456cd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build DataFrame, remove NaNs/dupes, basic text sanitation\n",
    "df = pd.DataFrame({'review': reviews})\n",
    "df.dropna(inplace=True)\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Strip non‑word characters and lowercase; ensure string dtype\n",
    "df['cleaned'] = (\n",
    "    df['review'].astype(str)\n",
    "    .str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "    .str.lower()\n",
    ")\n",
    "df_cleaned = df['cleaned'].tolist()\n",
    "\n",
    "print(f'Cleaned list length: {len(df_cleaned)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bdbc1d",
   "metadata": {},
   "source": [
    "## 5  Sentiment analysis – TextBlob *default* analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9aa765",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_default = {'positive': 0, 'negative': 0, 'neutral': 0}\n",
    "for txt in df_cleaned:\n",
    "    pol = TextBlob(txt).sentiment.polarity\n",
    "    if pol > 0:\n",
    "        counts_default['positive'] += 1\n",
    "    elif pol < 0:\n",
    "        counts_default['negative'] += 1\n",
    "    else:\n",
    "        counts_default['neutral'] += 1\n",
    "\n",
    "sizes = [counts_default[k] for k in ['positive', 'negative', 'neutral']]\n",
    "labels = ['Positive', 'Negative', 'Neutral']\n",
    "\n",
    "total = sum(sizes)\n",
    "if total == 0:\n",
    "    print('No sentiment data to plot (TextBlob default).')\n",
    "else:\n",
    "    filtered = [(s, l) for s, l in zip(sizes, labels) if s > 0]\n",
    "    sizes, labels = zip(*filtered)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, wedgeprops={'width':0.3})\n",
    "    ax.set_title('Sentiment Distribution (TextBlob Default)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9493986",
   "metadata": {},
   "source": [
    "## 6  Sentiment analysis – TextBlob *NaiveBayesAnalyzer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e6ba90",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_nb = {'pos': 0, 'neg': 0}\n",
    "for txt in df_cleaned:\n",
    "    try:\n",
    "        analysis = TextBlob(txt, analyzer=NaiveBayesAnalyzer()).sentiment\n",
    "        if analysis.classification == 'pos':\n",
    "            counts_nb['pos'] += 1\n",
    "        else:\n",
    "            counts_nb['neg'] += 1\n",
    "    except Exception as e:\n",
    "        print(f'Skipped a review due to: {e}')\n",
    "\n",
    "sizes = [counts_nb['pos'], counts_nb['neg']]\n",
    "labels = ['Positive', 'Negative']\n",
    "\n",
    "total = sum(sizes)\n",
    "if total == 0:\n",
    "    print('No sentiment data to plot (NaiveBayesAnalyzer).')\n",
    "else:\n",
    "    filtered = [(s, l) for s, l in zip(sizes, labels) if s > 0]\n",
    "    sizes, labels = zip(*filtered)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=90, wedgeprops={'width':0.3})\n",
    "    ax.set_title('Sentiment Distribution (NaiveBayesAnalyzer)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678f9ac0",
   "metadata": {},
   "source": [
    "## 7  WordCloud of top 20 words (stop‑words removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7958b80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text = ' '.join(df_cleaned)\n",
    "tokens = [w for w in all_text.split() if w.isalpha()]\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [w for w in tokens if w not in stop_words]\n",
    "\n",
    "if not filtered_tokens:\n",
    "    print('No words remaining after stop‑word removal.')\n",
    "else:\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    wc = WordCloud(width=800, height=400, max_words=20).generate(filtered_text)\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.imshow(wc, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd86abd",
   "metadata": {},
   "source": [
    "## 8  Conclusion\n",
    "- We successfully gathered and cleaned **{len}** reviews for *The Shawshank Redemption* via the free TMDB API.\n",
    "- TextBlob’s default analyzer showed the share of positive, negative, and neutral opinions, while NaiveBayesAnalyzer gave a simple positive/negative split.\n",
    "- The WordCloud highlighted the most frequent, meaningful words once stop‑words were removed.\n",
    "\n",
    "These steps demonstrate an end‑to‑end, cost‑free pipeline for sentiment analysis and basic NLP visualisation using openly available APIs and Python libraries."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
